# Confidence Calibration Guide

How to calibrate confidence levels for code review findings.

---

## The 80% Rule

**Only report issues with >= 80% confidence.**

This filters out:
- Stylistic preferences
- Uncertain concerns
- Opinions without evidence
- "Might be a problem" observations

---

## Confidence Levels Explained

### 90%+ Confidence: Definite Issues

**Criteria:**
- You can explain exactly why it's wrong
- You can show an input that triggers the bug
- The code violates documented requirements
- You've verified the issue exists

**You should feel:**
- Certain this is wrong
- Able to demonstrate the problem
- Confident the fix is correct

### Examples: 90%+ Confidence

| Finding | Why 90%+ |
|---------|----------|
| `unwrap()` on user input | Will panic on invalid input - can demonstrate |
| SQL without parameterization | Injection vulnerability - proven attack vector |
| Missing null check before deref | Will crash - can trace code path |
| Off-by-one in array access | Index out of bounds - can show exact input |
| Hardcoded secret in source | Security violation - visible in code |

**Example write-up:**

```markdown
### Critical: Panic on invalid configuration (Confidence: 95%)

**File:** `src/config.rs:42`

**Problem:** Using `unwrap()` on user-provided JSON field.

**Evidence:**
```rust
let value = json.get("required_field").unwrap();  // Line 42
```

If JSON doesn't contain "required_field", this panics. User can trigger by providing:
```json
{}
```

**Fix:**
```rust
let value = json.get("required_field")
    .ok_or_else(|| ConfigError::MissingField("required_field"))?;
```
```

---

### 80%+ Confidence: Likely Issues

**Criteria:**
- You're fairly certain but may not have proof
- The code contradicts established patterns
- The behavior is clearly unintended
- Similar code elsewhere handles it differently

**You should feel:**
- Confident enough to flag it
- Able to explain the concern
- Reasonable expectation of being correct

### Examples: 80%+ Confidence

| Finding | Why 80%+ |
|---------|----------|
| Missing error log before return | Pattern elsewhere logs; this doesn't |
| Clone where borrow would work | Performance concern, pattern analysis |
| Public function without docs | API contract unclear |
| Test missing edge case | Similar tests check this case |
| Timeout not configurable | Best practice for network calls |

**Example write-up:**

```markdown
### Important: Missing error context (Confidence: 85%)

**File:** `src/api.rs:78`

**Problem:** Error returned without context about what operation failed.

**Evidence:**
```rust
let response = client.get(url).send()?;  // Line 78
```

Other API calls in this file use:
```rust
let response = client.get(url).send()
    .context("Failed to fetch user profile")?;
```

This makes debugging harder when the request fails.

**Fix:**
```rust
let response = client.get(url).send()
    .context(format!("Failed to call {}", url))?;
```
```

---

### Below 80%: Don't Report (or Ask as Question)

**Criteria:**
- "This might be wrong"
- "I would have done it differently"
- "This feels off but I can't explain why"
- No evidence beyond intuition

**You should feel:**
- Uncertain
- Unable to demonstrate the problem
- Potentially just personal preference

### Examples: Below 80% (Don't Report)

| Observation | Why < 80% |
|-------------|-----------|
| "Could use a helper function" | Subjective, works as-is |
| "Variable name could be better" | Style preference |
| "Might cause performance issues" | No evidence, speculation |
| "I don't like this pattern" | Personal preference |
| "This is unusual" | Unusual != wrong |

**If you must mention (as question):**

```markdown
### Question: Is intentional? (Not an issue, just curious)

**File:** `src/parser.rs:120`

**Observation:** Function returns empty vec on error instead of Result.

I'm not flagging this as an issue since it may be intentional for this use case.
Was there a specific reason to return empty rather than propagate the error?
```

---

## Calibration Exercises

### Exercise 1: Classify These Findings

Decide if each is 90%+, 80%+, or <80%:

1. **`let _ = important_operation();`**
   - Answer: 80%+ (Discarding result, likely unintended)

2. **"This function is too long"**
   - Answer: <80% (Subjective, may be fine)

3. **`array[user_index]` without bounds check**
   - Answer: 90%+ (Can panic, demonstrable)

4. **"Could use `map` instead of `match`"**
   - Answer: <80% (Style preference)

5. **Missing `#[must_use]` on Result-returning function**
   - Answer: 80%+ (Established best practice)

6. **SQL query with string interpolation**
   - Answer: 90%+ (Security vulnerability)

7. **"Inconsistent brace style"**
   - Answer: <80% (Formatting, not a bug)

8. **Async function calling blocking I/O**
   - Answer: 90%+ (Will cause problems)

### Exercise 2: Rewrite as High-Confidence

Take this low-confidence observation and make it high-confidence:

**Low confidence:**
> "This error handling looks wrong"

**High confidence:**
> "Error on line 45 is silently discarded with `let _ = operation()`.
> If `operation()` fails, the caller receives no indication of failure.
> Other similar functions in this module propagate the error."

---

## Self-Check Questions

Before reporting an issue, ask yourself:

### For 90%+ Confidence

- [ ] Can I show exact input that causes the bug?
- [ ] Can I point to specific code that's wrong?
- [ ] Would any reasonable developer agree this is a bug?
- [ ] Is this objectively wrong, not just different?

### For 80%+ Confidence

- [ ] Can I explain WHY this is problematic?
- [ ] Does similar code elsewhere do it differently?
- [ ] Is there a documented pattern this violates?
- [ ] Would fixing this prevent a real problem?

### If You Answer "No" to Most

**Don't report it.** Or prefix with "Question:" rather than "Issue:"

---

## Common Calibration Mistakes

### Overconfidence Indicators

You're likely overconfident if:
- "Obviously wrong" but can't explain why
- "Everyone knows" but no citation
- Strong opinion but weak evidence
- Pattern "should" be different but works

### Underconfidence Indicators

You're likely underconfident if:
- Have evidence but hesitant to report
- Can demonstrate bug but calling it "possible issue"
- Matches known vulnerability pattern but "not sure"
- Other code does it right, this doesn't

---

## Reporting Template by Confidence

### 90%+ Template

```markdown
### [Critical/Important]: [Title] (Confidence: XX%)

**File:** `path/to/file.rs:line`

**Problem:** [Specific, provable issue]

**Evidence:** [Input that triggers, code that proves it]

**Fix:**
```rust
// Suggested fix
```
```

### 80%+ Template

```markdown
### [Important/Minor]: [Title] (Confidence: XX%)

**File:** `path/to/file.rs:line`

**Problem:** [Issue description]

**Evidence:** [Why you believe this is wrong]

**Suggested fix:**
```rust
// One approach
```
```

### Question Template (for <80%)

```markdown
### Question: [Topic]

**File:** `path/to/file.rs:line`

**Observation:** [What you noticed]

**Question:** [What you want to understand]

This is not a blocking issue, just seeking clarification.
```

---

## Kodo Integration

**Capture calibration learnings:**
```bash
kodo reflect --signal "High confidence pattern: <description>"
kodo reflect --signal "False positive avoided: <description>"
```

**Check past calibration:**
```bash
kodo query "review confidence"
kodo query "false positives"
```
